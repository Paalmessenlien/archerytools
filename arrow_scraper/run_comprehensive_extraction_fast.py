#!/usr/bin/env python3
"""
Fast comprehensive extraction that:
1. Skips image downloads unless vision extraction is needed
2. Uses consistent manufacturer names from config
"""

import asyncio
import json
import os
import sys
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import requests
from urllib.parse import urljoin, urlparse
from dotenv import load_dotenv

# Add parent directory to path
sys.path.append(str(Path(__file__).parent))

from crawl4ai import AsyncWebCrawler
from models import ArrowSpecification, SpineSpecification, ManufacturerData, ScrapingSession, ScrapingResult
from run_comprehensive_extraction import DirectLLMExtractor as OriginalExtractor
from content_pattern_learner import ContentPatternLearner

class FastDirectLLMExtractor(OriginalExtractor):
    """Optimized extractor with pattern learning for faster content extraction"""
    
    def __init__(self, api_key: str, manufacturer_name: str = None, skip_images: bool = True, enable_learning: bool = True, use_api: bool = True):
        super().__init__(api_key)
        self.manufacturer_name = manufacturer_name  # Use consistent name from config
        self.skip_images = skip_images  # Skip image downloads by default
        self.enable_learning = enable_learning
        self.use_api = use_api  # Whether to use DeepSeek API or not
        
        # Initialize pattern learner
        if enable_learning:
            self.pattern_learner = ContentPatternLearner()
        else:
            self.pattern_learner = None
        
    def extract_arrow_data(self, content: str, url: str) -> List[ArrowSpecification]:
        """Extract arrow data with pattern learning and consistent manufacturer naming"""
        
        # Use pattern learning to optimize content slice if available
        optimized_content = content
        if self.pattern_learner and self.manufacturer_name:
            optimization = self.pattern_learner.get_optimized_content_slice(url, content, self.manufacturer_name)
            if optimization:
                pattern_type, slice_start, slice_end = optimization
                optimized_content = content[slice_start:slice_end]
                print(f"üéØ Using learned {pattern_type} pattern: {len(optimized_content)} chars (vs {len(content)} original)")
        
        # Extract arrows based on mode
        arrows = []
        if self.use_api:
            # Use DeepSeek API for extraction
            arrows = super().extract_arrow_data(optimized_content, url)
        else:
            # Fast mode - no API calls, just return empty list for pattern learning
            print(f"‚ö° FAST MODE: Skipping API extraction, analyzing content structure only")
            # We still want to learn patterns from the content structure
            arrows = []
        
        # Learn from content structure (even if no arrows extracted in fast mode)
        if self.pattern_learner and self.manufacturer_name:
            try:
                # In fast mode, we learn from content structure even without extracted data
                self.pattern_learner.learn_successful_pattern(
                    url=url,
                    content=content,
                    manufacturer=self.manufacturer_name,
                    extraction_method="text",
                    extracted_data=arrows  # Empty list in fast mode, but still learns content patterns
                )
            except Exception as e:
                print(f"‚ö†Ô∏è  Pattern learning error: {e}")
        
        # Override manufacturer name with config value if provided
        if self.manufacturer_name and arrows:
            for arrow in arrows:
                arrow.manufacturer = self.manufacturer_name
                print(f"‚úÖ Using config manufacturer name: {self.manufacturer_name}")
        
        return arrows
    
    def download_image(self, image_url: str, manufacturer: str, model_name: str, image_type: str = "primary") -> str:
        """Download image and return local path, or just return URL if skipping"""
        if self.skip_images:
            # Return the URL instead of downloading
            return image_url
            
        # Original download logic
        return super().download_image(image_url, manufacturer, model_name, image_type)
    
    def finalize_learning(self):
        """Save learned patterns and show statistics"""
        if self.pattern_learner:
            try:
                self.pattern_learner.save_patterns()
                stats = self.pattern_learner.get_pattern_statistics()
                
                if stats.get('total_patterns', 0) > 0:
                    print(f"\nüß† Pattern Learning Summary:")
                    print(f"   ‚Ä¢ Total patterns learned: {stats['total_patterns']}")
                    print(f"   ‚Ä¢ Domains covered: {len(stats.get('by_domain', {}))}")
                    print(f"   ‚Ä¢ Manufacturers: {len(stats.get('by_manufacturer', {}))}")
                    
                    # Show most successful patterns
                    if stats.get('most_successful'):
                        print(f"   ‚Ä¢ Top patterns:")
                        for pattern in stats['most_successful'][:3]:
                            print(f"     - {pattern['domain']} ({pattern['pattern_type']}): {pattern['success_count']} uses")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Error finalizing pattern learning: {e}")


# Update the main.py update_all_manufacturers function
async def update_all_manufacturers_fast(deepseek_api_key: str, force_update: bool = False, enable_translation: bool = True):
    """Fast update that skips unnecessary image downloads"""
    
    print("üöÄ Starting FAST comprehensive manufacturer update...")
    print("‚ö° Image downloads disabled for non-vision extraction")
    print("==" * 30)
    
    # Import necessary modules
    from config_loader import ConfigLoader
    from arrow_database import ArrowDatabase
    from easyocr_carbon_express_extractor import EasyOCRCarbonExpressExtractor
    from deepseek_knowledge_extractor import DeepSeekKnowledgeExtractor
    from deepseek_translator import DeepSeekTranslator
    
    try:
        config = ConfigLoader()
        database = ArrowDatabase()
    except Exception as e:
        print(f"‚ùå Failed to initialize systems: {e}")
        return False
    
    manufacturer_names = config.get_manufacturer_names()
    
    start_time = time.time()
    total_manufacturers = len(manufacturer_names)
    successful_updates = 0
    total_arrows_found = 0
    
    print(f"üìä Updating {total_manufacturers} manufacturers from config...")
    print(f"üîÑ Force update: {'Yes' if force_update else 'No'}")
    print(f"üåç Translation: {'Enabled' if enable_translation else 'Disabled'}")
    print()
    
    for i, manufacturer_name in enumerate(manufacturer_names, 1):
        print(f"[{i}/{total_manufacturers}] üèπ Processing: {manufacturer_name}")
        print("-" * 40)
        
        try:
            # Check if manufacturer exists in database
            if not force_update:
                existing_count = database.get_arrows_by_manufacturer(manufacturer_name.lower())
                if existing_count and len(existing_count) > 0:
                    print(f"‚ÑπÔ∏è  Found {len(existing_count)} existing arrows - skipping")
                    print("   Use --force to update existing data")
                    print()
                    continue
            
            # Get URLs for this manufacturer
            all_urls = config.get_manufacturer_urls(manufacturer_name)
            if not all_urls:
                print(f"‚ö†Ô∏è  No URLs found for {manufacturer_name}")
                print()
                continue
                
            print(f"üìä Found {len(all_urls)} URLs to process")
            
            # Check if vision extraction is needed
            is_vision_based = config.is_vision_extraction(manufacturer_name)
            skip_images = not is_vision_based  # Only download images for vision-based extraction
            
            # Initialize extractors with manufacturer name
            text_extractor = FastDirectLLMExtractor(
                deepseek_api_key, 
                manufacturer_name=manufacturer_name,
                skip_images=skip_images
            )
            vision_extractor = None
            knowledge_extractor = DeepSeekKnowledgeExtractor(deepseek_api_key)
            translator = DeepSeekTranslator(deepseek_api_key)
            
            if is_vision_based:
                print(f"ü§ñ Vision extraction enabled - images WILL be downloaded")
            else:
                print(f"‚ö° Text extraction only - images will NOT be downloaded")
            
            # Initialize vision extractor if needed
            if is_vision_based:
                print(f"ü§ñ Initializing vision extractor for {manufacturer_name}...")
                vision_extractor = EasyOCRCarbonExpressExtractor()
                if vision_extractor.reader:
                    print("‚úÖ EasyOCR ready for image extraction")
                else:
                    print("‚ö†Ô∏è  EasyOCR not available, falling back to text extraction")
                    vision_extractor = None
            
            manufacturer_arrows = []
            failed_urls = []
            
            # Process URLs
            async with AsyncWebCrawler(verbose=False) as crawler:
                for j, url in enumerate(all_urls, 1):
                    print(f"   üìé [{j}/{len(all_urls)}] Processing URL...", end="")
                    
                    try:
                        # Crawl the page
                        result = await crawler.arun(url=url, bypass_cache=True)
                        
                        if not result.success:
                            print(" ‚ùå Crawl failed")
                            failed_urls.append(url)
                            continue
                        
                        print(f" ‚úì Crawled", end="")
                        
                        # Three-tier extraction system
                        arrows = []
                        
                        # Tier 1: Text extraction (with consistent manufacturer name)
                        arrows = text_extractor.extract_arrow_data(result.markdown, url)
                        
                        # Tier 2: Vision extraction (if available and needed)
                        if not arrows and vision_extractor:
                            print(" ‚Üí üñºÔ∏è  Vision...", end="")
                            arrows = vision_extractor.extract_vision_based_data(
                                result.html, result.markdown, url
                            )
                            # Override manufacturer name for vision-extracted arrows
                            for arrow in arrows:
                                arrow.manufacturer = manufacturer_name
                        
                        # Tier 3: Knowledge base fallback
                        if not arrows:
                            print(" ‚Üí üß† Knowledge...", end="")
                            arrows = knowledge_extractor.extract_from_failed_url(url, manufacturer_name)
                        
                        if arrows:
                            # Ensure all arrows have consistent manufacturer name
                            for arrow in arrows:
                                arrow.manufacturer = manufacturer_name
                            
                            # Translate arrows if needed
                            manufacturer_language = config.get_manufacturer_language(manufacturer_name)
                            if enable_translation and manufacturer_language and manufacturer_language != 'english':
                                print(f" ‚Üí üåç Translating from {manufacturer_language}...", end="")
                                translated_arrows = []
                                for arrow in arrows:
                                    translated_arrow = translator.translate_arrow_data(arrow, manufacturer_language)
                                    # Ensure manufacturer name stays consistent after translation
                                    translated_arrow.manufacturer = manufacturer_name
                                    translated_arrows.append(translated_arrow)
                                arrows = translated_arrows
                                print(" ‚úÖ")
                            
                            manufacturer_arrows.extend(arrows)
                            print(f" ‚Üí ‚úÖ {len(arrows)} arrows")
                        else:
                            print(" ‚Üí ‚ùå No data")
                            failed_urls.append(url)
                        
                        # Rate limiting
                        await asyncio.sleep(2)
                        
                    except Exception as e:
                        print(f" ‚Üí üí• Error: {str(e)[:30]}...")
                        failed_urls.append(url)
                        continue
            
            # Process results
            if manufacturer_arrows:
                arrow_count = len(manufacturer_arrows)
                total_arrows_found += arrow_count
                successful_updates += 1
                
                # Update database
                added_count = 0
                for arrow in manufacturer_arrows:
                    try:
                        database.add_arrow(arrow)
                        added_count += 1
                    except Exception as e:
                        print(f"‚ö†Ô∏è  Database error for {arrow.model_name}: {e}")
                
                print(f"‚úÖ {manufacturer_name}: {arrow_count} arrows extracted, {added_count} added to database")
                
                # Show unique models
                unique_models = set(arrow.model_name for arrow in manufacturer_arrows)
                print(f"   üìã Unique models: {len(unique_models)}")
                total_spines = sum(len(arrow.spine_specifications) for arrow in manufacturer_arrows)
                print(f"   üéØ Total spine specs: {total_spines}")
                
            else:
                print(f"‚ùå {manufacturer_name}: No arrows found")
                
            if failed_urls:
                print(f"   ‚ö†Ô∏è  Failed URLs: {len(failed_urls)}/{len(all_urls)}")
            
            # Add delay between manufacturers
            if i < total_manufacturers:
                print("‚è±Ô∏è  Waiting 5 seconds...")
                await asyncio.sleep(5)
            
        except Exception as e:
            print(f"‚ùå Error processing {manufacturer_name}: {e}")
            import traceback
            traceback.print_exc()
        
        print()
    
    # Summary
    duration = time.time() - start_time
    print("=" * 60)
    print("üìã FAST UPDATE SUMMARY")
    print("=" * 60)
    print(f"‚è±Ô∏è  Total time: {duration:.1f} seconds ({duration/60:.1f} minutes)")
    print(f"üè¢ Manufacturers processed: {successful_updates}/{total_manufacturers}")
    print(f"üèπ Total arrows found: {total_arrows_found}")
    print(f"‚úÖ Success rate: {(successful_updates/total_manufacturers*100):.1f}%")
    print(f"‚ö° Speed improvement: Image downloads skipped for non-vision manufacturers")
    print()
    
    # Database summary
    try:
        stats = database.get_statistics()
        total_db_arrows = stats.get('total_arrows', 0)
        total_db_manufacturers = stats.get('total_manufacturers', 0)
        print(f"üíæ Database now contains:")
        print(f"   ‚Ä¢ {total_db_arrows} arrows")
        print(f"   ‚Ä¢ {total_db_manufacturers} manufacturers")
    except Exception as e:
        print(f"üíæ Database summary unavailable: {e}")
    print()
    
    if successful_updates > 0:
        print("üéØ Fast update completed successfully!")
        return True
    else:
        print("‚ö†Ô∏è  No manufacturers were updated")
        return False


if __name__ == "__main__":
    # Test the fast extraction
    load_dotenv()
    api_key = os.getenv("DEEPSEEK_API_KEY")
    
    if not api_key:
        print("‚ùå DEEPSEEK_API_KEY not found in environment")
        sys.exit(1)
    
    # Run the fast update
    asyncio.run(update_all_manufacturers_fast(api_key, force_update=True))